{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip -q install langchain-groq duckduckgo-search\n",
    "!pip -q install -U langchain_community tiktoken langchainhub\n",
    "!pip -q install -U langchain langgraph tavily-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (0.3.22)\n",
      "Requirement already satisfied: langgraph in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: cassio in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (0.1.10)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain) (0.3.49)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain) (0.3.22)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain) (2.11.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph) (2.0.23)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph) (0.1.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph) (0.1.60)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: cassandra-driver<4.0.0,>=3.28.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from cassio) (3.29.2)\n",
      "Requirement already satisfied: numpy>=1.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from cassio) (2.2.4)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from cassandra-driver<4.0.0,>=3.28.0->cassio) (0.2.1.post1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.0)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio) (8.1.8)\n",
      "Requirement already satisfied: six in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio) (1.17.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from click->geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (0.3.20)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: langchainhub in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (0.1.21)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "Requirement already satisfied: langchain in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (0.3.22)\n",
      "Requirement already satisfied: langgraph in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (0.3.49)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (3.11.16)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (0.3.22)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain_community) (2.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain-groq) (0.21.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchainhub) (24.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchainhub) (2.32.0.20250328)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (2.11.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (0.115.12)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (3.23.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (4.13.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (1.31.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph) (2.0.23)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph) (0.1.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph) (0.1.60)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.46.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (1.33)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b1\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b1\n",
      "  Using cached opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.30.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Installing collected packages: opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\Users\\\\Shashwat\\\\Desktop\\\\Capstone 2\\\\env\\\\Lib\\\\site-packages\\\\chromadb\\\\test\\\\api\\\\test_collection.py'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langgraph cassio\n",
    "\n",
    "%pip install -U langchain_community tiktoken langchain-groq langchainhub chromadb langchain langgraph \n",
    "\n",
    "% pip install langchain_huggingface\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp311-cp311-win_amd64.whl (13.7 MB)\n",
      "     --------------------------------------- 13.7/13.7 MB 20.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\shashwat\\desktop\\capstone 2\\env\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langraph\n",
    "#### 3 Agents\n",
    "- Orchestrator\n",
    "- Narrative \n",
    "- Credit note\n",
    "\n",
    "#### Tools\n",
    "- Web\n",
    "- Stocks\n",
    "- Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()  # Load env variables from .env file\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# Docs to index\n",
    "urls=[\n",
    "\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\", \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-11m/\",\n",
    "]\n",
    "# Load documents from the web\n",
    "docs=[WebBaseLoader (url).load() for url in urls]\n",
    "doc_list=[item for sublist in docs for item in sublist] # Flattening the list\n",
    "# Splitting documents into smaller chunks\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder (chunk_size=500, chunk_overlap=0)\n",
    "docs_split = text_splitter.split_documents (doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 52 headlines.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# Generate embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# Store embeddings in Cassandra\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "faiss_vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "# Insert embeddings into the vector store \n",
    "faiss_vector_store.add_documents(docs_split)\n",
    "print(\"Inserted %i headlines.\" % len(docs_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain.indexes.vectorstore import VectorStoreIndexwrapper\n",
    "\n",
    "# faiss_vector_index = VectorStoreIndexwrapper(vectorstore = faiss_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0c752e0f-29bc-46f6-8259-56c1ab6d3c15', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Planning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes'),\n",
       " Document(id='e3d07afd-7ce4-4dc8-bf9c-dda30aad96da', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\"),\n",
       " Document(id='d30ad2a2-fd95-4cbb-aded-95898acadac2', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
       " Document(id='2dc6e5bb-d07a-4fbd-8861-9481233f7c1b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"Nlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\")]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever = faiss_vector_store.as_retriever()\n",
    "retriever.invoke(\"what is agent\", ConsistencyLevel=\"LOCAL_ONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shashwat\\Desktop\\Capstone 2\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3549: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Literal\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class RouteQuery (BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant output.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"credit_narrative\", \"credit_note\"] = Field(\n",
    "        ...,\n",
    "    description=\"Given a user question choose to route it to Credit Narrative or a Credit Note.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(groq_api_key = api_key, model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system = \"\"\"You are an expert at routing a user question to a credit narrative or credit note.\n",
    "The user would typicall ask for either a credit narrative or a credit note to be generated.\n",
    "If the user asks for a credit narrative to be generated use credit_narrative. Otherwise, use credit_note.\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    "\n",
    ")\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='credit_narrative'\n",
      "datasource='credit_note'\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "question_router.invoke(\n",
    "    {\"question\": \"Generate me a credit narrative for the company Ola Electrical\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "print(question_router.invoke({\"question\": \"Draft a credit note to issue a loan for Ola Electrical\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web Search\n",
    "\n",
    "#Duckduckgo\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "duckducksearch_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "\n",
    "# Tavily\n",
    "# import os\n",
    "# os.environ['TAVILY_API_KEY'] = \"<Your Tavily API Key here>\"\n",
    "# from langchain_core.tools import tool\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Stock Data\n",
    "\n",
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if \"POLYGON_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"POLYGON_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# from langchain_community.agent_toolkits.polygon.toolkit import PolygonToolkit\n",
    "# from langchain_community.utilities.polygon import PolygonAPIWrapper\n",
    "\n",
    "# from langchain import hub\n",
    "# from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "\n",
    "# polygon = PolygonAPIWrapper()\n",
    "# toolkit = PolygonToolkit.from_polygon_api_wrapper(polygon)\n",
    "\n",
    "# toolkit.get_tools()\n",
    "\n",
    "\n",
    "# instructions = \"\"\"You are an assistant.\"\"\"\n",
    "# base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "# prompt = base_prompt.partial(instructions=instructions)\n",
    "\n",
    "# agent = create_openai_functions_agent(llm, toolkit.get_tools(), prompt)\n",
    "\n",
    "# agent_executor = AgentExecutor(\n",
    "#     agent=agent,\n",
    "#     tools=toolkit.get_tools(),\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.0)\n",
    "tools = [YahooFinanceNewsTool()]\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_executor.invoke({\"input\": \"What is the recent new regarding MSFT?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def narrative_gen(state):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive credit narrative by querying multiple data sources:\n",
    "    1. Vector database for relevant internal documents\n",
    "    2. Web search for current market information\n",
    "    3. Stock data for financial metrics when relevant\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state containing at minimum a \"question\" key\n",
    "        \n",
    "    Returns:\n",
    "        dict: Enhanced state with added keys for retrieved information and the final narrative\n",
    "    \"\"\"\n",
    "    print(\"---CREDIT NARRATIVE GENERATION---\")\n",
    "    question = state[\"question\"]\n",
    "    narrative_data = {}\n",
    "    \n",
    "    # 1. Vector DB Retrieval - Get internal documents\n",
    "    try:\n",
    "        print(\"Retrieving internal documents...\")\n",
    "        vector_docs = retriever.invoke(question, ConsistencyLevel=\"LOCAL_ONE\")\n",
    "        narrative_data[\"internal_documents\"] = vector_docs\n",
    "    except Exception as e:\n",
    "        print(f\"Vector retrieval error: {e}\")\n",
    "        narrative_data[\"internal_documents\"] = []\n",
    "    \n",
    "    # 2. Web Search - Get current market information\n",
    "    try:\n",
    "        print(\"Performing web search...\")\n",
    "        web_results = duckducksearch_tool.run(f\"credit market {question}\")\n",
    "        narrative_data[\"web_information\"] = web_results\n",
    "    except Exception as e:\n",
    "        print(f\"Web search error: {e}\")\n",
    "        narrative_data[\"web_information\"] = \"No web information retrieved.\"\n",
    "    \n",
    "    # 3. Stock Data - Get financial metrics when applicable\n",
    "    try:\n",
    "        print(\"Fetching financial data...\")\n",
    "        # Extract potential company names or tickers from the question\n",
    "        # This is simplified - you might need more sophisticated entity extraction\n",
    "        words = question.split()\n",
    "        potential_tickers = [word.upper() for word in words if word.isalpha() and len(word) <= 5]\n",
    "        \n",
    "        stock_data = {}\n",
    "        if potential_tickers:\n",
    "            for ticker in potential_tickers[:3]:  # Limit to first 3 potential tickers\n",
    "                try:\n",
    "                    # Using the agent executor to get comprehensive stock information\n",
    "                    stock_result = agent_chain.invoke({\n",
    "                        \"input\": f\"Get latest financial metrics and credit indicators for {ticker}\"\n",
    "                    })\n",
    "                    stock_data[ticker] = stock_result[\"output\"]\n",
    "                except:\n",
    "                    continue\n",
    "        narrative_data[\"financial_metrics\"] = stock_data\n",
    "    except Exception as e:\n",
    "        print(f\"Stock data retrieval error: {e}\")\n",
    "        narrative_data[\"financial_metrics\"] = {}\n",
    "    \n",
    "    # Generate the comprehensive narrative by combining all sources\n",
    "    print(\"Generating comprehensive credit narrative...\")\n",
    "    \n",
    "    # Format the combined data for the narrative generation\n",
    "    combined_context = f\"\"\"\n",
    "    INTERNAL KNOWLEDGE BASE INFORMATION:\n",
    "    {format_documents(narrative_data.get('internal_documents', []))}\n",
    "    \n",
    "    CURRENT MARKET INFORMATION:\n",
    "    {narrative_data.get('web_information', 'No current market information available.')}\n",
    "    \n",
    "    FINANCIAL METRICS AND INDICATORS:\n",
    "    {format_financial_data(narrative_data.get('financial_metrics', {}))}\n",
    "    \"\"\"\n",
    "    # print(question)\n",
    "    # print(combined_context)\n",
    "    \n",
    "    # Use the LLM to generate the final narrative\n",
    "    narrative = llm.invoke(f\"\"\"\n",
    "    Based on the following information, generate a comprehensive credit narrative addressing this question:\n",
    "    \"{question}\"\n",
    "    \n",
    "    INFORMATION SOURCES:\n",
    "    {combined_context}\n",
    "    \n",
    "    Provide a well-structured credit narrative that integrates all relevant information from the sources above.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Return enhanced state\n",
    "    return {\n",
    "        \"documents\": narrative_data.get('internal_documents', []),\n",
    "        \"web_information\": narrative_data.get('web_information', \"\"),\n",
    "        \"financial_metrics\": narrative_data.get('financial_metrics', {}),\n",
    "        \"question\": question,\n",
    "        \"narrative\": narrative\n",
    "    }\n",
    "\n",
    "def format_documents(docs: List[Document]) -> str:\n",
    "    \"\"\"Format retrieved documents into a readable string\"\"\"\n",
    "    if not docs:\n",
    "        return \"No internal documents found.\"\n",
    "    \n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)\n",
    "        formatted.append(f\"Document {i+1}:\\n{content}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "def format_financial_data(financial_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"Format financial metrics data into a readable string\"\"\"\n",
    "    if not financial_data:\n",
    "        return \"No relevant financial metrics found.\"\n",
    "    \n",
    "    formatted = []\n",
    "    for ticker, data in financial_data.items():\n",
    "        formatted.append(f\"Financial data for {ticker}:\\n{data}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_gen(state):\n",
    "    \"\"\"\n",
    "    wiki search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---wikipedia---\")\n",
    "    print(\"---HELLO--\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "\n",
    "    # Wiki search\n",
    "    # docs = wiki.invoke({\"query\": question})\n",
    "    #print(docs[\"summary\"])\n",
    "    wiki_results = docs\n",
    "    wiki_results = Document(page_content=wiki_results)\n",
    "\n",
    "    return {\"documents\": wiki_results, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to credit narrative or credit tool.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.datasource == \"credit_narrative\":\n",
    "        print(\"---ROUTE QUESTION TO credit narrative---\")\n",
    "        return \"credit_narrative\"\n",
    "    elif source.datasource == \"credit_tool\":\n",
    "        print(\"---ROUTE QUESTION TO credit_tool---\")\n",
    "        return \"credit_tool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "class GraphState:\n",
    "    def __init__(self, input_data):\n",
    "        self.input_data = input_data  # Store input data for workflow\n",
    "        self.result = None  # Store final output\n",
    "\n",
    "    def update_result(self, result):\n",
    "        self.result = result\n",
    "\n",
    "\n",
    "workflow = StateGraph(dict)\n",
    "\n",
    "# Define nodes\n",
    "workflow.add_node(\"credit_narrative\", narrative_gen)\n",
    "workflow.add_node(\"credit_tool\", note_gen)\n",
    "\n",
    "# Build graph with routing logic\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"credit_narrative\": \"credit_narrative\",  \n",
    "        \"credit_tool\": \"credit_tool\" \n",
    "    }\n",
    ")\n",
    "\n",
    "# Fix the node names in edges\n",
    "workflow.add_edge(\"credit_narrative\", END)\n",
    "workflow.add_edge(\"credit_tool\", END)\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Display graph\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except Exception:\n",
    "# This requires some extra dependencies and is optional pass\n",
    "    print(\"Error\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO credit narrative---\n",
      "---CREDIT NARRATIVE GENERATION---\n",
      "Retrieving internal documents...\n",
      "Performing web search...\n",
      "Fetching financial data...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo get the latest financial metrics and credit indicators for a company, I need to access its financial news and data. \n",
      "\n",
      "Action: yahoo_finance_news\n",
      "Action Input: ME (assuming 'ME' is the ticker symbol for the company in question)\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo get the latest financial metrics and credit indicators for a company, I need to find a reliable source of financial news and data. \n",
      "\n",
      "Action: yahoo_finance_news\n",
      "Action Input: A\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo find the latest financial metrics and credit indicators for a company with the ticker FOR, I should first look for financial news and data related to this company.\n",
      "\n",
      "Action: yahoo_finance_news\n",
      "Action Input: FOR\u001b[0mGenerating comprehensive credit narrative...\n",
      "\"Node 'credit_narrative':\"\n",
      "'\\n---\\n'\n",
      "{'documents': [Document(id='c2c4b50c-33f2-48bc-b8cf-8a4bb0ee76a5', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasic Prompting\\n\\nZero-Shot\\n\\nFew-shot\\n\\nTips for Example Selection\\n\\nTips for Example Ordering\\n\\n\\n\\nInstruction Prompting\\n\\nSelf-Consistency Sampling\\n\\nChain-of-Thought (CoT)\\n\\nTypes of CoT prompts\\n\\nTips and Extensions\\n\\n\\nAutomatic Prompt Design\\n\\nAugmented Language Models\\n\\nRetrieval\\n\\nProgramming Language\\n\\nExternal APIs\\n\\n\\nCitation\\n\\nUseful Resources\\n\\nReferences\"), Document(id='f3935dd2-6b9b-47c6-ae55-3ae6a85b3a3e', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\"), Document(id='8f4b0da8-0e89-4c5e-8706-ff6c7b3dbf03', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\\n\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nMany studies looked into how to construct in-context examples to maximize the performance and observed that choice of prompt format, training examples, and the order of the examples can lead to dramatically different performance, from near random guess to near SoTA.\\nZhao et al. (2021) investigated the case of few-shot classification and proposed that several biases with LLM (they use GPT-3 in the experiments) contribute to such high variance: (1) Majority label bias exists if distribution of labels among the examples is unbalanced; (2) Recency bias refers to the tendency where the model may repeat the label at the end; (3) Common token bias indicates that LLM tends to produce common tokens more often than rare tokens. To conquer such bias, they proposed a method to calibrate the label probabilities output by the model to be uniform when the input string is N/A.\\nTips for Example Selection#\\n\\n\\nChoose examples that are semantically similar to the test example using $k$-NN clustering in the embedding space (Liu et al., 2021)\"), Document(id='c1abd896-9403-4feb-9f58-6abfd84c51d4', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Or\\n@article{weng2023prompt,\\n  title   = \"Prompt Engineering\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Mar\",\\n  url     = \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\\n}\\nUseful Resources#\\n\\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\\nLangChain, a library for combining language models with other components to build applications.\\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\\nlearnprompting.org\\nPromptPerfect\\nSemantic Kernel')], 'web_information': '141. By Anand Thirunellai Radhakrishnan, Senior Director and Saadat (Sid) Mubashar, Industry Practice Lead, Moody\\'s. I n an era of increasingly complex financial landscapes, corporate credit assessment is undergoing a radical transformation powered by generative artificial intelligence (GenAI). Traditional credit analysis has long relied on a multifaceted approach, drawing from company ... A loan narrative must be tailored to its audience, and the audience may change depending on the type of loan you\\'re seeking. The narrative around a new development project will be much different than the narrative around a renovation or a refinancing. Yes, a lot of the same information will be included in both of those loan proposals, but the ... A recently published whitepaper by Moody\\'s looked deeper into the transformative impact of artificial intelligence on the credit memo process, including the challenges and solutions to boost efficiency in banking operations.. The whitepaper, titled \"Transforming Banking Operations with AI: A Detailed Look at Automated Credit Memo\" delves into the current state of banking and the need of AI ... Credit application processes are the next-largest area of reported activity, along with controls and reporting: just over 40 percent of our respondents report ongoing or planned projects in both areas (Exhibit 2). Across business lines, respondents see slightly more potential for gen AI in wholesale than in retail credit. Credit analysts are the linchpins of the commercial banking sector, holding a pivotal role in the decision-making process that determines which businesses receive financing and under what terms. These professionals are tasked with scrutinizing the financial health of loan applicants, a process that involves a deep dive into financial statements, market trends, industry analyses, and the ...', 'financial_metrics': {}, 'question': 'Generate me a credit narrative for a corporate loan for Apple', 'narrative': AIMessage(content=\"**Comprehensive Credit Narrative for Apple's Corporate Loan**\\n\\nAs we navigate the increasingly complex financial landscape, Apple's creditworthiness is undergoing a transformation, driven by the integration of generative artificial intelligence (GenAI) in corporate credit assessment. This narrative is tailored to provide a comprehensive overview of Apple's credit profile, addressing the needs of our audience and highlighting the company's strengths, opportunities, and challenges.\\n\\n**Introduction**\\n\\nApple is a leading technology company with a strong brand presence, diverse product portfolio, and significant market share. The company's financial health, industry trends, and market position will be scrutinized to determine its creditworthiness. Our analysis will draw from traditional credit assessment methods, supplemented by insights from GenAI, to provide a more nuanced understanding of Apple's credit profile.\\n\\n**Financial Performance**\\n\\nAlthough specific financial metrics are not available, our analysis will focus on Apple's overall financial health, including revenue growth, profitability, and cash flow generation. The company's ability to maintain a strong financial position, invest in research and development, and return value to shareholders will be evaluated.\\n\\n**Industry Trends and Market Position**\\n\\nThe technology industry is highly competitive, with rapidly evolving trends and innovations. Apple's market position, brand loyalty, and ability to adapt to changing consumer preferences will be assessed. The company's diversification into new markets, such as services and wearables, will also be considered.\\n\\n**Credit Assessment**\\n\\nOur credit assessment will incorporate a multifaceted approach, drawing from company-specific information, industry trends, and market analysis. The transformative impact of GenAI on credit assessment will be considered, including the potential for increased efficiency and accuracy in the credit memo process. The challenges and solutions to boost efficiency in banking operations, as outlined in the Moody's whitepaper, will also be taken into account.\\n\\n**Risk Factors**\\n\\nThe credit narrative will also address potential risk factors, including market volatility, regulatory changes, and competitive pressures. Apple's ability to mitigate these risks, through strategic planning, diversification, and innovation, will be evaluated.\\n\\n**Conclusion**\\n\\nIn conclusion, this comprehensive credit narrative provides a well-structured overview of Apple's credit profile, integrating relevant information from traditional credit assessment methods and GenAI insights. The company's financial health, industry trends, and market position will be continuously monitored to ensure that our credit assessment remains accurate and up-to-date. As the credit landscape continues to evolve, our approach will adapt to incorporate new technologies, methodologies, and best practices, ensuring that Apple's creditworthiness is accurately reflected.\\n\\n**Recommendations**\\n\\nBased on our analysis, we recommend that lenders consider Apple's strong brand presence, diverse product portfolio, and significant market share when evaluating the company's creditworthiness. The integration of GenAI in credit assessment will provide a more nuanced understanding of Apple's credit profile, enabling lenders to make more informed decisions. Regular monitoring of the company's financial health, industry trends, and market position will be essential to ensure that our credit assessment remains accurate and relevant.\\n\\nBy adopting a comprehensive and forward-looking approach to credit assessment, lenders can better navigate the complexities of the financial landscape and make more informed decisions about Apple's creditworthiness. As the credit landscape continues to evolve, our approach will adapt to incorporate new technologies, methodologies, and best practices, ensuring that Apple's credit profile is accurately reflected.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 668, 'prompt_tokens': 1534, 'total_tokens': 2202, 'completion_time': 2.429090909, 'prompt_time': 0.105477431, 'queue_time': 0.06023495899999999, 'total_time': 2.53456834}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_72a5dc99ee', 'finish_reason': 'stop', 'logprobs': None}, id='run-b528bbdc-dcf5-4609-8812-3eb99daa10c1-0', usage_metadata={'input_tokens': 1534, 'output_tokens': 668, 'total_tokens': 2202})}\n",
      "[Document(id='c2c4b50c-33f2-48bc-b8cf-8a4bb0ee76a5', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasic Prompting\\n\\nZero-Shot\\n\\nFew-shot\\n\\nTips for Example Selection\\n\\nTips for Example Ordering\\n\\n\\n\\nInstruction Prompting\\n\\nSelf-Consistency Sampling\\n\\nChain-of-Thought (CoT)\\n\\nTypes of CoT prompts\\n\\nTips and Extensions\\n\\n\\nAutomatic Prompt Design\\n\\nAugmented Language Models\\n\\nRetrieval\\n\\nProgramming Language\\n\\nExternal APIs\\n\\n\\nCitation\\n\\nUseful Resources\\n\\nReferences\"),\n",
      " Document(id='f3935dd2-6b9b-47c6-ae55-3ae6a85b3a3e', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\"),\n",
      " Document(id='8f4b0da8-0e89-4c5e-8706-ff6c7b3dbf03', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\\n\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nMany studies looked into how to construct in-context examples to maximize the performance and observed that choice of prompt format, training examples, and the order of the examples can lead to dramatically different performance, from near random guess to near SoTA.\\nZhao et al. (2021) investigated the case of few-shot classification and proposed that several biases with LLM (they use GPT-3 in the experiments) contribute to such high variance: (1) Majority label bias exists if distribution of labels among the examples is unbalanced; (2) Recency bias refers to the tendency where the model may repeat the label at the end; (3) Common token bias indicates that LLM tends to produce common tokens more often than rare tokens. To conquer such bias, they proposed a method to calibrate the label probabilities output by the model to be uniform when the input string is N/A.\\nTips for Example Selection#\\n\\n\\nChoose examples that are semantically similar to the test example using $k$-NN clustering in the embedding space (Liu et al., 2021)\"),\n",
      " Document(id='c1abd896-9403-4feb-9f58-6abfd84c51d4', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Or\\n@article{weng2023prompt,\\n  title   = \"Prompt Engineering\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Mar\",\\n  url     = \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\\n}\\nUseful Resources#\\n\\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\\nLangChain, a library for combining language models with other components to build applications.\\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\\nlearnprompting.org\\nPromptPerfect\\nSemantic Kernel')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Run\n",
    "input_data = {\n",
    "\"question\": \"Generate me a credit narrative for a corporate loan for Apple\"\n",
    "}\n",
    "for output in app.stream(input_data):\n",
    "    for key, value in output.items():\n",
    "    # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "    # Optional: print full state at each node\n",
    "    # pprint.pprint(value[\"keys\"], indent-2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "# Final generation\n",
    "print(value)\n",
    "pprint(value['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jimmy, the first president to use his nickname in an official capacity, rather than his first name James. [195] Jimmy Cardigan, got the nickname after he wore a sweater instead of a suit one day [18] The Peanut Farmer, [196] he owned a peanut farm and fostered this image in his early campaigns, as a contrast to elite Washington insiders. The White House, official residence of the president of the United States, in July 2008. The president of the United States is the head of state and head of government of the United States, [1] indirectly elected to a four-year term via the Electoral College. [2] Under the U.S. Constitution, the officeholder leads the executive branch of the federal government and is the commander-in-chief of ... President John F. Kennedy, codename \"Lancer\" with First Lady Jacqueline Kennedy, codename \"Lace\". The United States Secret Service uses code names for U.S. presidents, first ladies, and other prominent persons and locations. [1] The use of such names was originally for security purposes and dates to a time when sensitive electronic communications were not routinely encrypted; today, the names ... The Irish Sun, It\\'s a fake Barack Obama\\'s brother posts forged document he claims is ex-president\\'s \\'Kenyan birth certificate,\\' March 11, 2017 Salon, Orly Taitz is at it again , Sept. 4, 2009 The first time the press mentioned Donald Trump\\'s name (the New York Times on January 28, 1973), there was no middle initial. He was simply \"Donald Trump,\" the brash son of Fred C. Trump.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# duckduckgo\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "duckducksearch_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# duckducksearch_tool.invoke(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. narrative_gen\n",
    "2. note_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [duckducksearch_tool, python_repl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "# This defines the object that is passed between each node\n",
    "# in the graph. We will create different nodes for each agent and tool\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    sender: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_executor = ToolExecutor(tools)\n",
    "def tool_node(state):\n",
    "    \"\"\"This runs tools in the graph\n",
    "    It takes in an agent action and calls that tool and returns the result.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    tool_input = json.loads(\n",
    "        last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "    )\n",
    "    # We can pass single-arg inputs by value\n",
    "    if len(tool_input) == 1 and \"__arg1\" in tool_input:\n",
    "        tool_input = next(iter(tool_input.values()))\n",
    "    tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"]\n",
    "    action = ToolInvocation(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_input,\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(\n",
    "        content=f\"{tool_name} response: {str(response)}\", name=action.tool\n",
    "    )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either agent can decide to end\n",
    "def router(state):\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        # The previus agent is invoking a tool\n",
    "        return \"call_tool\"\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return \"end\"\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3.3-8b-8192\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "    functions = [convert_to_openai_function(t) for t in tools]\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\\\\\\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    return prompt | llm.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create a node for a given agent\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    # We convert the agent output into a format that is suitable to append to the global state\n",
    "    if isinstance(result, FunctionMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        # Since we have a strict workflow, we can\n",
    "        # track the sender so we know who to pass to next.\n",
    "        \"sender\": name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent= create_agent(\n",
    "    llm,\n",
    "    [duckducksearch_tool],\n",
    "    system_message=\"You should provide accurate data for the chart generator to use.\",\n",
    ")\n",
    "vectordb_agent= create_agent(\n",
    "    llm,\n",
    "    [vectordb_search_tool],\n",
    "    system_message=\"Any charts you display will be visible by the user.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily\n",
    "import os\n",
    "os.environ['TAVILY_API_KEY'] = \"<Your Tavily API Key here>\"\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "def create_agent(llm, tools):\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    def chatbot(state: AgentState):\n",
    "        return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "    graph_builder = StateGraph(AgentState)\n",
    "    graph_builder.add_node(\"agent\", chatbot)\n",
    "\n",
    "    tool_node = ToolNode(tools=tools)\n",
    "    graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        tools_condition,\n",
    "    )\n",
    "    graph_builder.add_edge(\"tools\", \"agent\")\n",
    "    graph_builder.set_entry_point(\"agent\")\n",
    "    return graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m narrative_generation_agent = \u001b[43mcreate_agent\u001b[49m(GROQ_LLM, [web_search_tool])\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mweb_research_node\u001b[39m(state: MessagesState) -> Command[Literal[\u001b[33m\"\u001b[39m\u001b[33msupervisor\u001b[39m\u001b[33m\"\u001b[39m]]:\n\u001b[32m      4\u001b[39m     result = narrative_generation_agent.invoke(state)\n",
      "\u001b[31mNameError\u001b[39m: name 'create_agent' is not defined"
     ]
    }
   ],
   "source": [
    "narrative_generation_agent = create_agent(GROQ_LLM, [web_search_tool])\n",
    "\n",
    "def web_research_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = narrative_generation_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"web_researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mVectorStore\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[32m      2\u001b[39m     (\n\u001b[32m      3\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33mA vectorstore contains information about food recipes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m, ingredients used and cooking procedure\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     )\n\u001b[32m      7\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "class VectorStore(BaseModel):\n",
    "    (\n",
    "         \"A vectorstore contains information about food recipes\"\n",
    "        \", ingredients used and cooking procedure\"\n",
    "    )\n",
    "\n",
    "    query: str\n",
    "\n",
    "router_prompt_template = (\n",
    "    \"You are an expert in routing user queries to a VectorStore\\n\"\n",
    "    \"The VectorStore contains information on food recipes.\\n\"\n",
    "    'Note that if a query is not recipe related, you must output \"not food related\", don\\'t try to use any tool.\\n\\n'\n",
    "    \"query: {query}\"\n",
    ")\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_template(router_prompt_template)\n",
    "question_router = prompt | llm.bind_tools(tools=[VectorStore])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
